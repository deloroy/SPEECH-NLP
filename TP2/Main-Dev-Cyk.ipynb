{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il demande le renvoi\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(SENT (GRP1 (ADV Il) (VERB demande)) (GRP2 (ART le) (NOM renvoi)))\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "from pcfg import grammar_counts, set_all_symbols\n",
    "from oov import tagger\n",
    "\n",
    "def binarize_PCFG_grammar(grammar):\n",
    "    \n",
    "    binary_grammar = deepcopy(grammar)\n",
    "    #grammar with counts !!!\n",
    "    \n",
    "    #convert into Chomsky_normal_form\n",
    "    #cf. https://en.wikipedia.org/wiki/Chomsky_normal_form\n",
    "        \n",
    "    #no need for START RULE (tag 'SENT' is already always at the left)\n",
    "    #no need for TERM RULE (no nonsolitary terminals)\n",
    "    \n",
    "    #apply BIN RULE (eliminate right-hand sides with more than 2 nonterminals)\n",
    "    \n",
    "    max_idx_new_symbol = 0\n",
    "    \n",
    "    for (root_tag, rules) in grammar.items():\n",
    "        #root_tag is the left hand symbol of the grammar rule\n",
    "        #rules are the PCFC rules for derivation of root_tag\n",
    "\n",
    "        for (list_tags, proba) in rules.items():\n",
    "            #print(list_tags)\n",
    "            nb_consecutive_tags = len(list_tags)\n",
    "            \n",
    "            if nb_consecutive_tags>2:                \n",
    "\n",
    "                counts = binary_grammar[root_tag][list_tags]\n",
    "                del binary_grammar[root_tag][list_tags]\n",
    "                \n",
    "                symbol = \"NEW_\" + str(max_idx_new_symbol)\n",
    "                max_idx_new_symbol += 1\n",
    "                binary_grammar[root_tag][(list_tags[0],symbol)] = counts\n",
    "                #print(root_tag,list_tags[0],symbol)\n",
    "                for k in range(1,nb_consecutive_tags-2):\n",
    "                    new_symbol = \"NEW_\" + str(max_idx_new_symbol)\n",
    "                    max_idx_new_symbol += 1\n",
    "                    binary_grammar[symbol] = {(list_tags[k],new_symbol): counts}\n",
    "                    #print(symbol,list_tags[k],new_symbol)\n",
    "                    symbol = new_symbol\n",
    "                #print(symbol,list_tags[-2],list_tags[-1])\n",
    "                #print(\"\")\n",
    "                binary_grammar[symbol] = {(list_tags[-2],list_tags[-1]): counts}\n",
    "    \n",
    "    #no need for DEL or UNIT rules (no such cases)\n",
    "    \n",
    "    return binary_grammar, max_idx_new_symbol\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "if True: #test\n",
    "    binary_grammar = {\"SENT\":{(\"GRP1\",\"GRP2\"):1},\"GRP1\":{(\"ADV\",\"VERB\"):1},\"GRP2\":{(\"ART\",\"NOM\"):1}}\n",
    "    tagger = {\"Il\":{\"ADV\":1},\"demande\":{\"VERB\":1},\"le\":{\"ART\":1},\"renvoi\":{\"NOM\":1}}\n",
    "    set_all_symbols = [\"SENT\",\"GRP1\",\"GRP2\",\"ADV\",\"VERB\",\"ART\",\"NOM\",\"PUNCT\"]  #redefining variable\n",
    "    nb_tags = 8 \n",
    "    nb_all_symbols = len(set_all_symbols) #redefining variable\n",
    "    tag_to_idtag = {tag:i for (i,tag) in enumerate(set_all_symbols)}\n",
    "\n",
    "\n",
    "else:\n",
    "    binary_grammar_counts, max_idx_new_symbol = binarize_PCFG_grammar(grammar_counts)\n",
    "    binary_grammar = normalize_counts(binary_grammar_counts)\n",
    "    #print(binary_grammar)\n",
    "    #print(binary_grammar['SENT'])\n",
    "    #print(np.sum(list(binary_grammar['SENT'].values())))\n",
    "\n",
    "    new_symbols = [\"NEW_\"+str(s) for s in range(max_idx_new_symbol)]\n",
    "    set_all_symbols = list(set_all_symbols) + new_symbols  #redefining variable\n",
    "    nb_all_symbols = len(set_all_symbols) #redefining variable\n",
    "\n",
    "    tag_to_idtag = {tag:i for (i,tag) in enumerate(set_all_symbols)}\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "\n",
    "EPS = math.pow(10,-10)\n",
    "\n",
    "def compute_CYK_tables(sentence):\n",
    "    #(cf. https://en.wikipedia.org/wiki/CYK_algorithm)\n",
    "    #finding most likely symbol deriving each substring, for increasing length of substring (from 1 to length of the sentence)\n",
    "    #and storing each time the position of the cut and the grammar rule enabling to reach such most likely derivation\n",
    "    \n",
    "    nb_words = len(sentence)\n",
    "   \n",
    "    max_proba_derivation = np.zeros((nb_words,nb_words,nb_all_symbols))\n",
    "    #max_proba_derivation[s,l,a] is the maximum probability of\n",
    "    #a parsing where symbol a derives substring x_s...x_(s+l)\n",
    "    \n",
    "    split_reaching_max = np.zeros((nb_words,nb_words,nb_all_symbols,3))\n",
    "    #split_reaching_max[s,l,a,0] stores index cut\n",
    "    #split_reaching_max[s,l,a,1] stores symbol b\n",
    "    #split_reaching_max[s,l,a,2] stores symbol c\n",
    "    \n",
    "    #(i) b derives x_s...x_(s+cut), c derives x_(s+cut)...x_(s+l)\n",
    "    #and a rewrites bc (a->bc in the grammar)\n",
    "    \n",
    "    #(ii) the splitting <cut,b,c> defined by (i) is the one enabling\n",
    "    #to reach the maximum probability for a to derives  x_s...x_(s+l)\n",
    "    #(ie enabling to reach max_proba_derivation[s,l,a])\n",
    "\n",
    "    #probabilities of tags for unary strings (words)\n",
    "    for (position_word,word) in enumerate(sentence):\n",
    "        tags = tagger[word] #tagger(word)\n",
    "        for (tag, proba) in tags.items():\n",
    "            id_tag = tag_to_idtag[tag]\n",
    "            max_proba_derivation[position_word,0,id_tag] = proba\n",
    "            \n",
    "    print(max_proba_derivation[:,0,:])\n",
    "    \n",
    "    for l in range(1, nb_words):\n",
    "        #we will consider symbols deriving strings of length l+1...\n",
    "        \n",
    "        for s in range(nb_words-l):\n",
    "            #... and starting at index s of the sentence\n",
    "            \n",
    "            for cut in range(0,l): \n",
    "                #... and such that the symbol can rewrite as two symbols AB\n",
    "                #with A deriving substring until index cut included, and B deriving substring from index cut+1\n",
    "                \n",
    "                for (root_tag, rules) in binary_grammar.items():\n",
    "                    #root_tag is the left hand symbol of the grammar rule\n",
    "                    #rules are the PCFC rules for derivation of root_tag\n",
    "                    \n",
    "                    idx_root_tag = tag_to_idtag[root_tag]\n",
    "                    \n",
    "                    for (split, proba) in rules.items():\n",
    "                        #root_tag can rewrite split[0]split[1] with probability proba\n",
    "                        \n",
    "                        if len(split)==2: #disregard rules A->B, consider only A->BC\n",
    "                            \n",
    "                            idx_left_tag = tag_to_idtag[split[0]] #idx of left split tag\n",
    "                            idx_right_tag = tag_to_idtag[split[1]] #idx of right split tag\n",
    "                                                        \n",
    "                            proba_decomposition = proba\n",
    "                            proba_decomposition *= max_proba_derivation[s,cut,idx_left_tag]\n",
    "                            proba_decomposition *= max_proba_derivation[s+cut+1,l-cut-1,idx_right_tag]\n",
    "                            \n",
    "                            if proba_decomposition > max_proba_derivation[s,l,idx_root_tag]:\n",
    "                                #therefore, we found a new decomposition <cut,split[0],split[1]>\n",
    "                                #reaching a highest probability for root_tag to derive substring x_s...x_(s+l)\n",
    "\n",
    "                                max_proba_derivation[s,l,idx_root_tag] = proba_decomposition\n",
    "                                split_reaching_max[s,l,idx_root_tag,0] = cut\n",
    "                                split_reaching_max[s,l,idx_root_tag,1] = idx_left_tag\n",
    "                                split_reaching_max[s,l,idx_root_tag,2] = idx_right_tag\n",
    "    \n",
    "        print(max_proba_derivation[:,l,:])\n",
    "                            \n",
    "    return max_proba_derivation, split_reaching_max.astype(int)\n",
    "\n",
    "#Rq for report : max_proba_derivation is non zero if there exists a triplet such that both are non zero and ...\n",
    "\n",
    "\n",
    "def parse_substring(s,l,idx_root_tag, sentence, max_proba_derivation, split_reaching_max):\n",
    "    #parse substring beginning at index s of sentence, of length l+1, and tagged as idx_root_tag\n",
    "    \n",
    "    nb_words = max_proba_derivation.shape[0]\n",
    "        \n",
    "    if l==0: #void string        \n",
    "        return sentence[s]\n",
    "    \n",
    "    else: #split enabling to reach max_proba_derivation[s,l,idx_root_tag]\n",
    "        cut = split_reaching_max[s,l,idx_root_tag,0]\n",
    "        idx_left_tag = split_reaching_max[s,l,idx_root_tag,1]\n",
    "        idx_right_tag = split_reaching_max[s,l,idx_root_tag,2]\n",
    "        \n",
    "        left_tag = set_all_symbols[idx_left_tag]\n",
    "        right_tag = set_all_symbols[idx_right_tag]\n",
    "        \n",
    "        #print(l,cut,l-cut)\n",
    "                    \n",
    "        return [[left_tag,parse_substring(s, cut, idx_left_tag, sentence, max_proba_derivation, split_reaching_max)],\n",
    "                [right_tag,parse_substring(s+cut+1, l-cut-1, idx_right_tag, sentence, max_proba_derivation, split_reaching_max)]]\n",
    "        \n",
    "def remove_artificial_symbols(parsing_dico):\n",
    "    if type(parsing_dico)==str:\n",
    "        return parsing_dico\n",
    "    else:\n",
    "        new_parsing_dico = []\n",
    "        for el in parsing_dico:\n",
    "            root_tag = el[0]\n",
    "            rules = el[1]\n",
    "            if tag_to_idtag[root_tag]>=nb_tags: #artificial symbol\n",
    "                dico = remove_artificial_symbols(rules)\n",
    "                for el2 in dico:\n",
    "                    new_parsing_dico.append([el2[0], el[1]])\n",
    "            else:\n",
    "                new_parsing_dico.append([root_tag, rules])\n",
    "        return new_parsing_dico\n",
    "            \n",
    "def parse(sentence):\n",
    "\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    nb_words = len(sentence)\n",
    "    \n",
    "    max_proba_derivation, split_reaching_max = compute_CYK_tables(sentence)    \n",
    "        \n",
    "    #idx_root_tag = np.argmax(max_proba_derivation[0,nb_words,:])\n",
    "    idx_root_tag = tag_to_idtag[\"SENT\"]\n",
    "    #rq ca devrait etre toujours S_0 à ce point !!!\n",
    "    \n",
    "    parsing_dico = parse_substring(0,nb_words-1,idx_root_tag, sentence, max_proba_derivation, split_reaching_max)\n",
    "    \n",
    "    res = remove_artificial_symbols(parsing_dico)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def reformat_parsing(parsing):\n",
    "    #converting parsing stored as a dictionnary into the required format (with nested brackets)\n",
    "    \n",
    "    if type(parsing)==str:\n",
    "        return parsing\n",
    "\n",
    "    else:\n",
    "        string=\"\"\n",
    "        for el in parsing:\n",
    "            root_tag = el[0]\n",
    "            parsing_substring = el[1]\n",
    "            string = string + \"(\" + root_tag + \" \" + reformat_parsing(parsing_substring) + \")\" + \" \"\n",
    "        string = string[:-1]\n",
    "        return string    \n",
    "\n",
    "def parser(sentence):\n",
    "    return \"(SENT \" + reformat_parsing(parse(sentence)) + \")\"\n",
    "\n",
    "if True:\n",
    "    # sentences_test = [sentence(postag) for postag in corpus_test]\n",
    "    sent = \"Il demande le renvoi\"\n",
    "    print(sent)\n",
    "    print(parser(sent))\n",
    "    #print(parser(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
